% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/DTRtree.R
\name{DTRtree}
\alias{DTRtree}
\title{Tree-based Reinforcement Learning for estimating optimal DTR.}
\usage{
DTRtree(
  Y,
  A,
  H,
  pis.hat = NULL,
  m.method = c("AIPW", "randomForest"),
  mus.reg = NULL,
  depth = 5,
  lambda.pct = 0.05,
  minsplit = 20,
  lookahead = F
)
}
\arguments{
\item{Y}{A vector of outcome of interest.}

\item{A}{A vector of observed treatment options.}

\item{H}{A matrix of covariates before assigning final treatment, excluding previous treatment variables.}

\item{pis.hat}{Estimated propensity score matrix.}

\item{m.method}{Method for calculating estimated conditional mean.}

\item{mus.reg}{Regression-based conditional mean outcome.}

\item{depth}{Maximum tree depth.}

\item{lambda.pct}{Minimal percent change in purity measure for split.}

\item{minsplit}{Minimal node size.}

\item{lookahead}{Whether or not to look into a further step of splitting to find the best split.}
}
\description{
a tree-based reinforcement learning (T-RL) method to directly
estimate optimal DTRs in a multi-stage multi-treatment setting. At
each stage, T-RL builds an unsupervised decision tree that directly handles
the problem of optimization with multiple treatment comparisons, through a
purity measure constructed with augmented inverse probability weighted estimators.
}
